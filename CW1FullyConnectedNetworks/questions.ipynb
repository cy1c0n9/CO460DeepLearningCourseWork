{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1: ML basics and fully-connected networks\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Please submit a version of this notebook containing your answers on CATe as *CW1*. Write your answers in the cells below each question.\n",
    "\n",
    "We recommend that you work on the Ubuntu workstations in the lab. This assignment and all code were only tested to work on these machines. In particular, we cannot guarantee compatibility with Windows machines and cannot promise support if you choose to work on a Windows machine.\n",
    "\n",
    "You can work from home and use the lab workstations via ssh (for list of machines: https://www.doc.ic.ac.uk/csg/facilities/lab/workstations). \n",
    "\n",
    "Once logged in, run the following commands in the terminal to set up a Python environment with all the packages you will need.\n",
    "\n",
    "    export PYTHONUSERBASE=/vol/bitbucket/nuric/pypi\n",
    "    export PATH=/vol/bitbucket/nuric/pypi/bin:$PATH\n",
    "\n",
    "Add the above lines to your `.bashrc` to have these enviroment variables set automatically each time you open your bash terminal.\n",
    "\n",
    "Any code that you submit will be expected to run in this environment. Marks will be deducted for code that fails to run.\n",
    "\n",
    "Run `jupyter-notebook` in the coursework directory to launch Jupyter notebook in your default browser.\n",
    "\n",
    "DO NOT attempt to create a virtualenv in your home folder as you will likely exceed your file quota.\n",
    "\n",
    "**DEADLINE: 7pm, Tuesday 5th February, 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "1. Describe two practical methods used to estimate a supervised learning model's performance on unseen data. Which strategy is most commonly used in most deep learning applications, and why?\n",
    "2. Suppose that you have reason to believe that your multi-layer fully-connected neural network is overfitting. List four things that you could try to improve generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\**ANSWERS FOR PART 1 IN THIS CELL*\\*   \n",
    "   \n",
    "1.  There are k-fold cross-validation and hold-out validation   \n",
    "    * K-fold Cross-Validation: K-fold Cross-Validation assesses how well the result of statistic analysis will generalize a independent dataset. Cross-Validation contain multiple rounds, and each round involves partitioning the samples of data into several complimentary subsets, left one subset alone and perform training on the rest subsets, and validating the training result by predicting test result on the left-alone subset. Each round we choose one subset toleft alone, repeat rounds until every subset has been used as test set.   \n",
    "    * Hold-Out Method(Validation): It is a kind of simplified Cross Validation. Randomly split dataset into a training and a testing partitions. Use training set to train and tset set to test the performance This will result a very quick estimate of performance.   \n",
    "    * K-fold Cross validation is the most commonly used, because hold-out method is to simple to give a good estimate for the neural network, and some other method like left-one-out method and bootstrapping are only work well on small dataset. Besides, k-fold cross validation gives a very good valitation and work well in hyper-parameters choosing.   \n",
    "    \n",
    "2.   \n",
    "        * add drop out layers within the network\n",
    "        * add L2-norm weight penalty\n",
    "        * reduce the depth or width of the network (thus reduce # parameters, and complexity of the model)\n",
    "        * add more data to training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "1. Why can gradient-based learning be difficult when using the sigmoid or hyperbolic tangent functions as hidden unit activation functions in deep, fully-connected neural networks?\n",
    "2. Why is the issue that arises in the previous question less of an issue when using such functions as output unit activation functions, provided that an appropriate loss function is used?\n",
    "3. What would happen if you initialize all the weights to zero in a multi-layer fully-connected neural network and attempt to train your model using gradient descent? What would happen if you did the same thing for a logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\**ANSWERS FOR PART 2 IN THIS CELL*\\*   \n",
    "1. As networks' weight received an update according to the partial derivative of the error function, weight will be update to a portion to the derivative of the activation function. Using sigmoid or hyperbolic tangent as activation functions cause a problem that when the back-propagated error function value is close to 0 or 1, the gradient will be vanishingly small, preventing weight from changing its value. This is called gradient vanishing problem.   \n",
    "    And this effect will be really a problem of the hidden unit, the gradients of the network's output with respect to the parameters in the early layers become extremely small, because at each hidden layer, the activation functions are trying to map large input into small region, thus large change in the input will result rather small change of the output. For earlier layer, zoom-in may happen several times. This make training difficult.   \n",
    "2. As descussed in previous question, when we only consider the output layer, the zoom-in effect only take place once, which cause less of an issue.   \n",
    "3.   \n",
    "        * In gradient descent, if zero is set to all weights, the training may tend to stuck on local minima, and all neuron tend to follow same gradient and end up with doing the same things.   \n",
    "        * In logistic regression, the globle minima may always be reached as it is convex function. And it is not a issue as logistic regression is simply y = Wx + b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "In this part, you will use PyTorch to implement and train a multinomial logistic regression model to classify MNIST digits.\n",
    "\n",
    "Restrictions:\n",
    "* You must use (but not modify) the code provided in `utils.py`. **This file is deliberately not documented**; read it carefully as you will need to understand what it does to complete the tasks.\n",
    "* You are NOT allowed to use the `torch.nn` module.\n",
    "\n",
    "Please insert your solutions to the following tasks in the cells below:\n",
    "1. Complete the `MultinomialLogisticRegressionClassifier` class below by filling in the missing parts (expected behaviour is prescribed in the documentation):\n",
    "    * The constructor\n",
    "    * `forward`\n",
    "    * `parameters`\n",
    "    * `l1_weight_penalty`\n",
    "    * `l2_weight_penalty`\n",
    "\n",
    "2. The default hyperparameters for `MultilayerClassifier` and `run_experiment` have been deliberately chosen to produce poor results. Experiment with different hyperparameters until you are able to get a test set accuracy above 92% after a maximum of 10 epochs of training. However, DO NOT use the test set accuracy to tune your hyperparameters; use the validation loss / accuracy. You can use any optimizer in `torch.optim`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import normal\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# *CODE FOR PART 3.1 IN THIS CELL*\n",
    "class MultinomialLogisticRegressionClassifier:\n",
    "    def __init__(self, weight_init_sd=100.0):\n",
    "        \"\"\"\n",
    "        Initializes model parameters to values drawn from the Normal\n",
    "        distribution with mean 0 and standard deviation `weight_init_sd`.\n",
    "        \"\"\"\n",
    "        self.weight_init_sd = weight_init_sd\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        m = normal.Normal(0, self.weight_init_sd)\n",
    "        self.param = autograd.Variable(m.sample((784, 10)), requires_grad=True)\n",
    "        self.bias = autograd.Variable(m.sample((1, 10)), requires_grad=True)\n",
    "        \n",
    "        self.params = [self.param, self.bias]\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the model.\n",
    "        \n",
    "        Expects `inputs` to be a Tensor of shape (batch_size, 1, 28, 28) containing\n",
    "        minibatch of MNIST images.\n",
    "        \n",
    "        Inputs should be flattened into a Tensor of shape (batch_size, 784),\n",
    "        before being fed into the model.\n",
    "        \n",
    "        Should return a Tensor of logits of shape (batch_size, 10).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        batch_size, _, _, _ = inputs.size()\n",
    "        inputs = inputs.view(batch_size, 784)\n",
    "        z = torch.mm(inputs, self.param) + self.bias\n",
    "        # z = xTw\n",
    "        z -= torch.max(z, dim=1)[0].view(batch_size, -1)\n",
    "        # z = z - max(z) for each line\n",
    "        exp = torch.exp(z)\n",
    "        # e^z\n",
    "        sums = torch.sum(exp, dim=1).view(batch_size, -1)\n",
    "        # sum(e^zi)\n",
    "        y = z - torch.log(sums)\n",
    "        # y = z - log(sum(e^zi))\n",
    "        return y\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Should return an iterable of all the model parameter Tensors.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        for p in self.params:\n",
    "            yield self.param\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        \n",
    "    def l1_weight_penalty(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the L1 norm of the model's weight vector (i.e. sum\n",
    "        of absolute values of all model parameters).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return torch.norm(self.param, p=1) + torch.norm(self.bias, p=1)\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def l2_weight_penalty(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the L2 weight penalty (i.e. \n",
    "        sum of squared values of all model parameters).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return torch.norm(self.param, p=2) + torch.norm(self.bias, p=2)\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training...\n",
      "Train set:\tAverage loss: 0.4914, Accuracy: 0.8575\n",
      "Validation set:\tAverage loss: 0.3654, Accuracy: 0.8938\n",
      "\n",
      "Epoch 1: training...\n",
      "Train set:\tAverage loss: 0.3380, Accuracy: 0.9029\n",
      "Validation set:\tAverage loss: 0.3223, Accuracy: 0.9070\n",
      "\n",
      "Epoch 2: training...\n",
      "Train set:\tAverage loss: 0.3152, Accuracy: 0.9112\n",
      "Validation set:\tAverage loss: 0.3116, Accuracy: 0.9127\n",
      "\n",
      "Epoch 3: training...\n",
      "Train set:\tAverage loss: 0.3038, Accuracy: 0.9141\n",
      "Validation set:\tAverage loss: 0.3128, Accuracy: 0.9095\n",
      "\n",
      "Epoch 4: training...\n",
      "Train set:\tAverage loss: 0.2966, Accuracy: 0.9167\n",
      "Validation set:\tAverage loss: 0.3006, Accuracy: 0.9145\n",
      "\n",
      "Epoch 5: training...\n",
      "Train set:\tAverage loss: 0.2915, Accuracy: 0.9177\n",
      "Validation set:\tAverage loss: 0.2950, Accuracy: 0.9162\n",
      "\n",
      "Epoch 6: training...\n",
      "Train set:\tAverage loss: 0.2882, Accuracy: 0.9195\n",
      "Validation set:\tAverage loss: 0.2923, Accuracy: 0.9180\n",
      "\n",
      "Epoch 7: training...\n",
      "Train set:\tAverage loss: 0.2848, Accuracy: 0.9200\n",
      "Validation set:\tAverage loss: 0.2859, Accuracy: 0.9210\n",
      "\n",
      "Test set:\tAverage loss: 0.2814, Accuracy: 0.9216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# *CODE FOR PART 3.2 IN THIS CELL - EXAMPLE WITH DEFAULT PARAMETERS PROVIDED *\n",
    "model = MultinomialLogisticRegressionClassifier(weight_init_sd=0.35)\n",
    "res = run_experiment(\n",
    "    model,\n",
    "    optimizer=optim.SGD(model.parameters(), 0.075, momentum=0.65),\n",
    "    train_loader=train_loader_0,\n",
    "    val_loader=val_loader_0,\n",
    "    test_loader=test_loader_0,\n",
    "    n_epochs=8,\n",
    "    l1_penalty_coef=0.0,\n",
    "    l2_penalty_coef=0.0035,\n",
    "    suppress_output=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "\n",
    "In this part, you will use PyTorch to implement and train a multi-layer fully-connected neural network to classify MNIST digits.\n",
    "\n",
    "Your network must have three hidden layers with 128, 64, and 32 hidden units respectively.\n",
    "\n",
    "The same restrictions as in Part 3 apply.\n",
    "\n",
    "Please insert your solutions to the following tasks in the cells below:\n",
    "1. Complete the `MultilayerClassifier` class below by filling in the missing parts of the following methods (expected behaviour is prescribed in the documentation):\n",
    "\n",
    "    * The constructor\n",
    "    * `forward`\n",
    "    * `parameters`\n",
    "    * `l1_weight_penalty`\n",
    "    * `l2_weight_penalty`\n",
    "\n",
    "2. The default hyperparameters for `MultilayerClassifier` and `run_experiment` have been deliberately chosen to produce poor results. Experiment with different hyperparameters until you are able to get a test set accuracy above 97% after a maximum of 10 epochs of training. However, DO NOT use the test set accuracy to tune your hyperparameters; use the validation loss / accuracy. You can use any optimizer in `torch.optim`.\n",
    "\n",
    "3. Describe an alternative strategy for initializing weights that may perform better than the strategy we have used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *CODE FOR PART 4.1 IN THIS CELL*\n",
    "\n",
    "class MultilayerClassifier:\n",
    "    def __init__(self, activation_fun=\"sigmoid\", weight_init_sd=1.0):\n",
    "        \"\"\"\n",
    "        Initializes model parameters to values drawn from the Normal\n",
    "        distribution with mean 0 and standard deviation `weight_init_sd`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.activation_fun = activation_fun\n",
    "        self.weight_init_sd = weight_init_sd\n",
    "\n",
    "        if self.activation_fun == \"relu\":\n",
    "            self.activation = F.relu\n",
    "        elif self.activation_fun == \"sigmoid\":\n",
    "            self.activation = torch.sigmoid\n",
    "        elif self.activation_fun == \"tanh\":\n",
    "            self.activation = torch.tanh\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        m = normal.Normal(0, self.weight_init_sd)\n",
    "        input_layer = 784\n",
    "        layer1 = 128\n",
    "        layer2 = 64\n",
    "        layer3 = 32\n",
    "        layer4 = 10\n",
    "        self.param1 = autograd.Variable(m.sample((input_layer, layer1)),\n",
    "                                        requires_grad=True)\n",
    "        self.param2 = autograd.Variable(m.sample((layer1, layer2)),\n",
    "                                        requires_grad=True)\n",
    "        self.param3 = autograd.Variable(m.sample((layer2, layer3)),\n",
    "                                        requires_grad=True)\n",
    "        self.param4 = autograd.Variable(m.sample((layer3, layer4)),\n",
    "                                        requires_grad=True)\n",
    "        self.bias1 = autograd.Variable(m.sample((1, layer1)),\n",
    "                                       requires_grad=True)\n",
    "        self.bias2 = autograd.Variable(m.sample((1, layer2)),\n",
    "                                       requires_grad=True)\n",
    "        self.bias3 = autograd.Variable(m.sample((1, layer3)),\n",
    "                                       requires_grad=True)\n",
    "        self.bias4 = autograd.Variable(m.sample((1, layer4)),\n",
    "                                       requires_grad=True)\n",
    "        self.params = [self.param1, self.param2, self.param3, self.param4,\n",
    "                       self.bias1, self.bias2, self.bias3, self.bias4]\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the model.\n",
    "        \n",
    "        Expects `inputs` to be Tensor of shape (batch_size, 1, 28, 28) containing\n",
    "        minibatch of MNIST images.\n",
    "        \n",
    "        Inputs should be flattened into a Tensor of shape (batch_size, 784),\n",
    "        before being fed into the model.\n",
    "        \n",
    "        Should return a Tensor of logits of shape (batch_size, 10).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        batch_size, _, _, _ = inputs.size()\n",
    "        inputs = inputs.view(batch_size, 784)\n",
    "        z0 = torch.mm(inputs, self.param1) + self.bias1\n",
    "        y0 = self.activation(z0)    #128\n",
    "        \n",
    "        z1 = torch.mm(y0, self.param2) + self.bias2\n",
    "        y1 = self.activation(z1)    #64\n",
    "        \n",
    "        z2 = torch.mm(y1, self.param3) + self.bias3\n",
    "        y2 = self.activation(z2)    #32\n",
    "        \n",
    "        z3 = torch.mm(y2, self.param4) + self.bias4\n",
    "        z3 -= torch.max(z3, dim=1)[0].view(batch_size, -1)\n",
    "        # z = z - max(z) for each line\n",
    "        exp = torch.exp(z3)\n",
    "        # e^z\n",
    "        sums = torch.sum(exp, dim=1).view(batch_size, -1)\n",
    "        # sum(e^zi)\n",
    "        y3 = z3 - torch.log(sums)\n",
    "        # y = z - log(sum(e^zi))\n",
    "        # 10\n",
    "        return y3\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Should return an iterable of all the model parameter Tensors.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        for p in self.params:\n",
    "            yield p\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        \n",
    "    \n",
    "    def l1_weight_penalty(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the L1 norm of the model's weight vector (i.e. sum\n",
    "        of absolute values of all model parameters).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        res = 0\n",
    "        for x in self.params:\n",
    "            res += torch.norm(x, p=1)\n",
    "        return res \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def l2_weight_penalty(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the L2 weight penalty (i.e. \n",
    "        sum of squared values of all model parameters).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        res = 0\n",
    "        for x in self.params:\n",
    "            res += torch.norm(x, p=2)\n",
    "        return res \n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training...\n",
      "Train set:\tAverage loss: 0.3336, Accuracy: 0.8964\n",
      "Validation set:\tAverage loss: 0.1752, Accuracy: 0.9468\n",
      "\n",
      "Epoch 1: training...\n",
      "Train set:\tAverage loss: 0.1589, Accuracy: 0.9522\n",
      "Validation set:\tAverage loss: 0.1902, Accuracy: 0.9410\n",
      "\n",
      "Epoch 2: training...\n",
      "Train set:\tAverage loss: 0.1262, Accuracy: 0.9629\n",
      "Validation set:\tAverage loss: 0.1285, Accuracy: 0.9590\n",
      "\n",
      "Epoch 3: training...\n",
      "Train set:\tAverage loss: 0.1077, Accuracy: 0.9681\n",
      "Validation set:\tAverage loss: 0.1149, Accuracy: 0.9653\n",
      "\n",
      "Epoch 4: training...\n",
      "Train set:\tAverage loss: 0.0986, Accuracy: 0.9711\n",
      "Validation set:\tAverage loss: 0.1124, Accuracy: 0.9645\n",
      "\n",
      "Epoch 5: training...\n",
      "Train set:\tAverage loss: 0.0900, Accuracy: 0.9737\n",
      "Validation set:\tAverage loss: 0.1197, Accuracy: 0.9635\n",
      "\n",
      "Epoch 6: training...\n",
      "Train set:\tAverage loss: 0.0845, Accuracy: 0.9755\n",
      "Validation set:\tAverage loss: 0.1084, Accuracy: 0.9663\n",
      "\n",
      "Epoch 7: training...\n",
      "Train set:\tAverage loss: 0.0829, Accuracy: 0.9753\n",
      "Validation set:\tAverage loss: 0.0963, Accuracy: 0.9698\n",
      "\n",
      "Epoch 8: training...\n",
      "Train set:\tAverage loss: 0.0776, Accuracy: 0.9777\n",
      "Validation set:\tAverage loss: 0.0996, Accuracy: 0.9715\n",
      "\n",
      "Test set:\tAverage loss: 0.0903, Accuracy: 0.9720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# *CODE FOR PART 4.2 IN THIS CELL - EXAMPLE WITH DEFAULT PARAMETERS PROVIDED *\n",
    "\n",
    "model = MultilayerClassifier(activation_fun='relu', weight_init_sd=0.20)\n",
    "res = run_experiment(\n",
    "    model,\n",
    "    optimizer=optim.SGD(model.parameters(), 0.073, momentum=0.7),\n",
    "    train_loader=train_loader_0,\n",
    "    val_loader=val_loader_0,\n",
    "    test_loader=test_loader_0,\n",
    "    n_epochs=9,\n",
    "    l1_penalty_coef=0.0,\n",
    "    l2_penalty_coef=0.014,\n",
    "    suppress_output=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\**ANSWERS FOR PART 4.3 IN THIS CELL*\\*   \n",
    "3. He initialization\n",
    "    Simply multiply random initialization for layer l with sqrt(2 / size_of_layer_l-1)\n",
    "    This work well with sigmoid activation function  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
